---
title: "class08"
author: "Edward Vo (A16215241)"
format: pdf
---

## Outline
Today we will apply the machine learning methods we introduced in the last class on breast cancer biopsy data from fine needle aspiration (FNA)

Preparing the data
```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
```

```{r}
# We can use -1 here to remove the first column (wisc.df is answer key)
wisc.data <- wisc.df[,-1]
```

```{r}
# Create diagnosis vector for later 
diagnosis <- wisc.df$diagnosis
```

## Exploratory data analaysis

# Question 1
```{r}
dim(wisc.data)
```
569 observations

# Question 2
```{r}
table(diagnosis)
```
212 are malignant

# Question 3
```{r}
x<- colnames(wisc.df)
length(grep("_mean", x))
```
10 variables are suffixed with "_mean"


## PCA

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale=TRUE)

# Need to scale since some columns are measured with different units and therefore have different means & variances
```

```{r}
# Look at summary of results
summary(wisc.pr)
```

# Question 4
44.3% of variance is captured by PC1

# Question 5
3 PCs are required to describe at least 70% of data

# Question 6
7 PCs are required to describe at least 90% of data

# Question 7
```{r}
biplot(wisc.pr)
```
Very messy plot & too many overlaps


```{r}
# Scatter plot observations by components 1 and 2

plot(x=wisc.pr$x[,1], y=wisc.pr$x[,2], col=as.factor(diagnosis), xlab = "PC1", ylab = "PC2")
```

# Question 8
```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = as.factor(diagnosis), xlab = "PC1", ylab = "PC3")
```
There is more overlap in graph of PC3:1 than PC2:1
Black = benign
Red = malignant

Making ggplot
```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + aes(PC1, PC2, col=diagnosis) + geom_point()
```

## Variance explained

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- wisc.pr$sdev^2 / sum(wisc.pr$sdev^2)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
## ggplot based graph
# install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

## Communicating PCA results

# Question 9
```{r}
wisc.pr$rotation[,1]
```
Concave points mean: -0.26085376

# Question 10
5 PCs are required to describe at least 80% of data

## Hierarchical clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)

# Calculate Euclidean distance
data.dist <- dist(data.scaled)

# Hierarchical clustering with complete linkage
wisc.hclust <- hclust(data.dist)
```

# Question 11
```{r}
plot(wisc.hclust)
abline(h = 19, col="red", lty=2)
```
Height = 19

## Selecting number of clusters

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)

# Compare membership to diagnosis
table(wisc.hclust.clusters, diagnosis)
```

# Question 12
```{r}
# Any better clustering?
new <- cutree(wisc.hclust, k=2)
table(new, diagnosis)
```
Less messy

## Using different methods
```{r}
complete <- hclust(data.dist, method="complete")
plot(complete)

single <- hclust(data.dist, method="single")
plot(single)

average <- hclust(data.dist, method="average")
plot(average)

ward.d2 <- hclust(data.dist, method="ward.D2")
plot(ward.d2)
```
# Question 13
Favorite is single because since it does a 1-by-1 approach, it gives slanted clusters (even though it's hard to draw associations and have a clear abline)

## K-means clustering

```{r}
wisc.km <- kmeans(scale(wisc.data), centers = 2, nstart = 20)
table(wisc.km$cluster, diagnosis)
```

# Question 14
It does a better job in separating B to M by proportion of the cluster

## Combining methods

Use PCA results instead of original data
```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method = "ward.D2")
plot (wisc.pr.hclust)
```

Generate 2 cluster groups from this hclust object
```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps)
```

```{r}
table(diagnosis)
table(diagnosis, grps)
```
B1 false positive
M2 false negative

# Question 15
```{r}
table(grps, diagnosis)
```
Good sensitivity